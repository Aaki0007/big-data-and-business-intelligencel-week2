{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd65d3c",
   "metadata": {},
   "source": [
    "\n",
    "# Session 2 – **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "This notebook guides you through a practical, end‑to‑end EDA workflow.  \n",
    "It is a warm up, after you finished it successfully, repeat the process for your own data and work through the steps.\n",
    "\n",
    "**What you'll do**\n",
    "1. Load and inspect your dataset\n",
    "2. Summarize numeric & categorical variables\n",
    "3. Explore missingness and duplicates\n",
    "4. Visualize distributions, relationships, and correlations\n",
    "5. Capture insights in short Markdown notes\n",
    "6. Draft 2–3 KPI ideas informed by your EDA\n",
    "\n",
    "**Note:** This notebook intentionally focuses on *EDA only*.  \n",
    "> Cleaning/feature engineering strategies (e.g., imputation/outlier handling) will be done in the next session.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb9907",
   "metadata": {},
   "source": [
    "You might need to download files from:\n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/GTNEJD&version=1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af658a",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "- Update `DATA_PATH` to your chosen dataset (CSV).  \n",
    "- If the file is not found, a small **synthetic demo dataset** will be generated so you can still run the notebook end‑to‑end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "# ---- Path to your CSV ----\n",
    "# If you have the Harvard PeopleSuN dataset in your data folder from last session, copy it\n",
    "# in the data folder of this week and set the path like:\n",
    "DATA_PATH = \"../data/peoplesun_hh_anon.csv\"  \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9c4c5",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load Data\n",
    "- Try to read the CSV at `DATA_PATH`.\n",
    "- If not found, we build a small **synthetic dataset** to let you proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6515d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def make_synthetic_demo(n=1000):\n",
    "#     # Toy dataset with mixed types: numeric, categorical, date, and some missingness\n",
    "#     dates = pd.date_range(\"2021-01-01\", periods=n, freq=\"D\")\n",
    "#     region = np.random.choice([\"North\",\"South\",\"East\",\"West\"], size=n, p=[0.25,0.25,0.25,0.25])\n",
    "#     income = np.random.lognormal(mean=10, sigma=0.5, size=n)  # heavily skewed\n",
    "#     pop_density = np.random.gamma(shape=2.0, scale=50, size=n)\n",
    "#     electrified = np.random.binomial(n=1, p=np.clip(0.3 + 0.001*(pop_density) + 0.00001*(income), 0.05, 0.95))\n",
    "#     # insert some missingness\n",
    "#     income[np.random.choice(n, size=n//15, replace=False)] = np.nan\n",
    "#     pop_density[np.random.choice(n, size=n//20, replace=False)] = np.nan\n",
    "#     # a few duplicates\n",
    "#     df = pd.DataFrame({\n",
    "#         \"date\": dates,\n",
    "#         \"region\": region,\n",
    "#         \"income\": income,\n",
    "#         \"population_density\": pop_density,\n",
    "#         \"electrified\": electrified\n",
    "#     })\n",
    "#     df = pd.concat([df, df.sample(5, random_state=RANDOM_SEED)], ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH,low_memory=False)\n",
    "    source_note = f\"Loaded dataset from: {DATA_PATH}\"\n",
    "# else:\n",
    "#     df = make_synthetic_demo(n=1200)\n",
    "#     source_note = \"Using synthetic demo dataset (file not found).\"\n",
    "\n",
    "source_note, df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46792c0",
   "metadata": {},
   "source": [
    "\n",
    "## 2) First Look\n",
    "**Questions**\n",
    "- What do rows represent? What does each column mean?\n",
    "- Do we recognize numeric vs categorical vs datetime columns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d905bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(source_note)\n",
    "print(\"\\nShape:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nInfo:\")\n",
    "print(df.info())\n",
    "#display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2ccfd",
   "metadata": {},
   "source": [
    "To make your EDA more interpretable:\n",
    "\n",
    "### a) Load and inspect the codebook\n",
    "\n",
    "If you have something like peoplesun_codebook.csv or .xlsx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192bc842",
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook = pd.read_excel(\"../data/peoplesun_hh_odk_codebook.xlsx\")\n",
    "display(codebook.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab695e6",
   "metadata": {},
   "source": [
    "### b) Build a quick lookup dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a4080",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip(codebook[\"name\"], codebook[\"label\"]))\n",
    "df_renamed = df.rename(columns=mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199c937",
   "metadata": {},
   "source": [
    "You have now replaced the code for questions with the full question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571df81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa1da3",
   "metadata": {},
   "source": [
    "Note that some columns are staying as codes, these are flattened options that were made out of answers to the last question.\n",
    "Example: 211. Can you list ALL of the income sources of your household?\n",
    "Then the following collumns represent options 1 to 14. The options can be found in choicebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a7f58",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Data Types & Basic Hygiene (EDA-only)\n",
    "- Identify numeric, categorical, datetime columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a664e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types\n",
    "numeric_cols = df_renamed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "datetime_cols = df_renamed.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "categorical_cols = [c for c in df_renamed.columns if c not in numeric_cols + datetime_cols]\n",
    "\n",
    "print(\"Numeric:\", numeric_cols)\n",
    "# Print a line to separate long lists\n",
    "print(\"-\" * 40)\n",
    "print(\"Datetime:\", datetime_cols)\n",
    "print(\"-\" * 40)\n",
    "print(\"Categorical/Other:\", categorical_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c462c9b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Descriptive Statistics\n",
    "Use `.describe()` for numeric and a custom summary for categorical columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_summary = df_renamed[numeric_cols].describe().T if numeric_cols else pd.DataFrame()\n",
    "categorical_summary = pd.DataFrame({\n",
    "    \"n_unique\": [df_renamed[c].nunique(dropna=True) for c in categorical_cols],\n",
    "    \"top_values\": [df_renamed[c].value_counts(dropna=False).head(5).to_dict() for c in categorical_cols]\n",
    "}, index=categorical_cols) if categorical_cols else pd.DataFrame()\n",
    "\n",
    "display(numeric_summary.head(5))\n",
    "display(categorical_summary.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f03f09",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Missingness Overview\n",
    "- Which columns have missing values? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing = df_renamed.isna().mean().sort_values(ascending=False)\n",
    "missing = (missing * 100).round(2).rename(\"%missing\")\n",
    "display(missing.to_frame().head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7cc92",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Duplicate Awareness\n",
    "- How many duplicated rows exist?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203252eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dup_count = int(df_renamed.duplicated().sum())\n",
    "total = len(df_renamed)\n",
    "print(f\"Duplicated rows: {dup_count} / {total} ({dup_count/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625176b",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Distributions (Numeric)\n",
    "- Histograms help reveal shape, skew, and potential outliers.\n",
    "> Tip: For skewed variables (e.g., income), you can inspect both linear and log scales. We don't have any here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde27dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_histograms(frame, columns, bins=30, logscale_candidates=None):\n",
    "    for col in columns:\n",
    "        data = frame[col].dropna()\n",
    "        if data.empty:\n",
    "            continue\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(data, bins=bins)\n",
    "        plt.title(f\"Histogram: {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_histograms(df_renamed, numeric_cols[1:5], bins=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bb27a",
   "metadata": {},
   "source": [
    "You see that most of numerical columns are actually boleans or not relevant as histograms, we use smarter approach to visualize our columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_boolish(s: pd.Series):\n",
    "    # True booleans or 0/1 only (ignoring NaN)\n",
    "    if s.dtype == bool:\n",
    "        return True\n",
    "    if np.issubdtype(s.dtype, np.number):\n",
    "        uniq = pd.unique(s.dropna())\n",
    "        return set(uniq).issubset({0, 1})\n",
    "    # string-y 'yes/no/true/false/1/0'\n",
    "    if s.dtype == object:\n",
    "        vals = s.dropna().astype(str).str.lower().unique()\n",
    "        return set(vals).issubset({\"0\",\"1\",\"true\",\"false\",\"yes\",\"no\"})\n",
    "    return False\n",
    "\n",
    "def is_continuous_numeric(s: pd.Series, small_card_max=12):\n",
    "    if not np.issubdtype(s.dropna().dtype, np.number):\n",
    "        return False\n",
    "    nunq = s.nunique(dropna=True)\n",
    "    # treat as continuous if reasonably many distinct values\n",
    "    return nunq > small_card_max\n",
    "\n",
    "def weighted_counts(series, weights=None):\n",
    "    if weights is None:\n",
    "        vc = series.value_counts(dropna=False)\n",
    "        return (vc / vc.sum() * 100).sort_values(ascending=False)  # percent\n",
    "    dfw = pd.DataFrame({\"val\": series})\n",
    "    dfw[\"_w\"] = weights\n",
    "    out = dfw.groupby(\"val\", dropna=False)[\"_w\"].sum()\n",
    "    return (out / out.sum() * 100).sort_values(ascending=False)\n",
    "\n",
    "def smart_univariate_plot(df, col, weight_col=None, bins=30, top=12, likert_order=None):\n",
    "    s = df[col]\n",
    "    w = df[weight_col] if weight_col and weight_col in df.columns else None\n",
    "\n",
    "    # 1) Boolean / multi-select dummy\n",
    "    if is_boolish(s):\n",
    "        # Normalize different encodings to True/False\n",
    "        if s.dtype != bool:\n",
    "            s = s.astype(str).str.lower().isin([\"1\",\"true\",\"yes\"])\n",
    "        pct = weighted_counts(s.fillna(False), weights=w)\n",
    "        plt.figure(figsize=(5,3.5))\n",
    "        plt.bar(pct.index.astype(str), pct.values)\n",
    "        plt.ylabel(\"% of records\")\n",
    "        plt.title(f\"{col} (boolean)\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return\n",
    "\n",
    "    # 2) Continuous numeric\n",
    "    if is_continuous_numeric(s):\n",
    "        data = s.dropna().values\n",
    "        if data.size == 0: \n",
    "            return\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(data, bins=bins)\n",
    "        plt.title(f\"Histogram: {col}\")\n",
    "        plt.xlabel(col); plt.ylabel(\"Count\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "        # optional log view if highly skewed (skewness > 1)\n",
    "        skew = pd.Series(data).skew()\n",
    "        if np.isfinite(skew) and skew > 1 and (data > 0).mean() > 0.95:\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.hist(np.log1p(data[data>0]), bins=bins)\n",
    "            plt.title(f\"Histogram (log1p): {col}\")\n",
    "            plt.xlabel(f\"log1p({col})\"); plt.ylabel(\"Count\"); plt.tight_layout(); plt.show()\n",
    "        return\n",
    "\n",
    "    # 3) Small-cardinality numeric (coded categories) OR text/object\n",
    "    if likert_order is not None and set(pd.unique(s.dropna())) <= set(likert_order):\n",
    "        # ordered bars for Likert (e.g., [1,2,3,4,5])\n",
    "        ordered = pd.Categorical(s, categories=likert_order, ordered=True)\n",
    "        pct = weighted_counts(ordered, weights=w)\n",
    "    else:\n",
    "        pct = weighted_counts(s.astype(\"object\"), weights=w)\n",
    "\n",
    "    pct = pct.head(top)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.bar([str(i) for i in pct.index], pct.values)\n",
    "    plt.xticks(rotation=35, ha=\"right\")\n",
    "    plt.ylabel(\"% of records\")\n",
    "    plt.title(f\"{col} (top {top})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_eda_overview(df, cols=None, weight_col=None, bins=30, top=12):\n",
    "    \"\"\"Plot a sensible chart per column; if cols=None, sample a few from each type.\"\"\"\n",
    "    if cols is None:\n",
    "        # Heuristics to pick a manageable subset automatically\n",
    "        boolish = [c for c in df.columns if is_boolish(df[c])]\n",
    "        continuous = [c for c in df.columns if is_continuous_numeric(df[c])]\n",
    "        smallcat = [c for c in df.columns \n",
    "                    if (not is_boolish(df[c])) and (not is_continuous_numeric(df[c]))]\n",
    "\n",
    "        cols = []\n",
    "        cols += boolish[:6]\n",
    "        cols += continuous[:6]\n",
    "        cols += smallcat[:8]\n",
    "\n",
    "    for c in cols:\n",
    "        try:\n",
    "            smart_univariate_plot(df, c, weight_col=weight_col, bins=bins, top=top)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {c}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4e7798",
   "metadata": {},
   "source": [
    "### Now plot using the above defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bee658",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eda_overview(df_renamed, weight_col=\"natweight\", bins=30, top=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6276622",
   "metadata": {},
   "source": [
    "### Or explicitly pick columns you care about:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"106. What is your age?\",\n",
    "]\n",
    "plot_eda_overview(df_renamed, cols=cols, weight_col=\"natweight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b52c9",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Numeric vs Categorical\n",
    "- Compare distributions of a numeric variable **across categories** (group summaries).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0e0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grouped_summary(frame, num_col, cat_col):\n",
    "    g = frame[[num_col, cat_col]].dropna().groupby(cat_col)[num_col]\n",
    "    summary = pd.DataFrame({\n",
    "        \"count\": g.count(),\n",
    "        \"mean\": g.mean(),\n",
    "        \"median\": g.median(),\n",
    "        \"std\": g.std()\n",
    "    }).sort_values(\"mean\", ascending=False)\n",
    "    return summary\n",
    "\n",
    "# Print grouped summaries for the first few combinations\n",
    "for cat in categorical_cols[:2]:\n",
    "    for num in numeric_cols[:3]:\n",
    "        print(f\"\\n--- {num} by {cat} ---\")\n",
    "        display(grouped_summary(df_renamed, num, cat).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eade13b",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Correlation Matrix (for continious numeric Only)\n",
    "- Use with care: correlation ≠ causation.\n",
    "- Still, helpful for spotting redundant features or strong linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5808326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only continuous numeric columns from your existing numeric_cols list\n",
    "cont_cols = [c for c in numeric_cols if is_continuous_numeric(df_renamed[c])]\n",
    "cont_cols = cont_cols[:10]  # keep the first 5 for visualization\n",
    "\n",
    "if len(cont_cols) >= 2:\n",
    "    corr = df_renamed[cont_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(corr, interpolation=\"nearest\")\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.colorbar()\n",
    "    # Optional labels:\n",
    "    # ticks = range(len(corr.columns))\n",
    "    # plt.xticks(ticks, corr.columns, rotation=90)\n",
    "    # plt.yticks(ticks, corr.columns)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    display(corr.round(3))\n",
    "else:\n",
    "    print(\"Not enough continuous numeric columns to correlate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b134a8f",
   "metadata": {},
   "source": [
    "You could also experiment with seaborn sns.pairplot() to see nice visualisations that match two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4458e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "cont_cols = [c for c in numeric_cols if is_continuous_numeric(df_renamed[c])]\n",
    "\n",
    "# Create the pair plot\n",
    "g = sns.pairplot(df_renamed[cont_cols[:5]], diag_kind=\"kde\")\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    # ticks\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "    ax.tick_params(axis=\"y\", labelrotation=0)\n",
    "\n",
    "    # axis titles (labels)\n",
    "    ax.set_xlabel(\"\")\n",
    "    #ax.set_ylabel(ax.get_ylabel(), rotation=45,  ha=\"right\", va=\"center\", labelpad=10)\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "g.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b9efa",
   "metadata": {},
   "source": [
    "\n",
    "## 10) Time Trends\n",
    "- If you had a datetime column which we don't in the PeopleSuN data: Aggregate by day/week/month and plot trends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_time_series(frame, date_col, value_col, freq=\"M\"):\n",
    "    sub = frame[[date_col, value_col]].dropna().copy()\n",
    "    if sub.empty:\n",
    "        print(f\"No data to plot for {value_col}.\")\n",
    "        return\n",
    "    sub = sub.set_index(date_col).sort_index().resample(freq).mean()\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(sub.index, sub[value_col])\n",
    "    plt.title(f\"Time Trend ({value_col}, resampled {freq})\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(value_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if datetime_cols and numeric_cols:\n",
    "    date_col = datetime_cols[0]\n",
    "    for v in numeric_cols[:3]:\n",
    "        plot_time_series(df_renamed, date_col, v, freq=\"M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd03e59",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Geographic Hints\n",
    "If you have **region/country** columns, consider:  \n",
    "- Choropleth (later in Streamlit), or  \n",
    "- Per‑region summary tables now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) keep only continuous numeric columns (not coded ints / dummies)\n",
    "cont_cols = [c for c in numeric_cols if is_continuous_numeric(df_renamed[c])]\n",
    "geo_like = [c for c in df_renamed.columns if any(k in c.lower() for k in [\"zone\",\"lga\",\"eaid\"])]\n",
    "for geo in geo_like:\n",
    "    for num in cont_cols[:2]:\n",
    "        print(f\"\\nAverage {num} by {geo}:\")\n",
    "        display(df_renamed.groupby(geo, dropna=False)[num].mean().sort_values(ascending=True).to_frame().head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33bbf1",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Write Down 2–3 Insights (Markdown)\n",
    "Use this cell to note **what surprised you**, **potential data issues**, and **early hypotheses**.\n",
    "\n",
    "- Insight 1: …  \n",
    "- Insight 2: …  \n",
    "- Hypothesis / Next question: …  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afe403",
   "metadata": {},
   "source": [
    "\n",
    "## 13) KPI Drafts (Driven by EDA)\n",
    "Fill in a first draft of KPIs **based on variables that are present and meaningful** in your dataset.\n",
    "\n",
    "| KPI Name | Definition | Formula (words) | Python Expression (sketch) |\n",
    "|---|---|---|---|\n",
    "| Example: Electrification Rate | % of electrified households | electrified / total_households | `df['electrified'] / df['total_households']` |\n",
    "|  |  |  |  |\n",
    "|  |  |  |  |\n",
    "\n",
    "> Keep KPIs **simple and measurable**. We’ll refine them after cleaning/feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e0f41",
   "metadata": {},
   "source": [
    "\n",
    "## 14) What We'll Tackle Next (Feature Engineering Preview)\n",
    "- Data type fixes (categorical, datetime)  \n",
    "- Missing value strategies (when to impute vs. drop)  \n",
    "- Outlier detection & robust statistics  \n",
    "- Scaling/encoding and derived features for your KPIs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197fbc5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
